{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction to Explainable AI using the Iris Dataset\n",
    "\n",
    "In this assignment, we delve into the realm of Explainable AI (XAI) using the well-known Iris dataset. Our focus will be on understanding and interpreting machine learning models through various explainers.\n",
    "\n",
    "1. Data Augmentation:\n",
    "The Iris dataset, known for its limited size, is enhanced through data augmentation. This process involves the injection of random noise into the dataset, thereby increasing its volume and variability without straying far from its original characteristics.\n",
    "\n",
    "2. Training the Autoencoder:\n",
    "We embark on training an autoencoder, which is a crucial step in our experiment. The training is conducted exclusively on the 'normal' class data. To bolster the autoencoder's ability to differentiate between normal and anomalous data, we introduce artificially generated anomalies. These anomalies are random data points that do not correlate with the other classes. The scaling of the data is tailored based on these normal observations, setting the foundation for effective anomaly detection.\n",
    "\n",
    "3. Classifier Development:\n",
    "Using the trained autoencoder, we construct a classifier. Its primary role is to distinguish between the normal class (Versicolor) and the anomalous classes (Setosa and Virginica). This classifier is fundamentally based on a loss threshold, a critical measure that differentiates normal data from anomalies.\n",
    "\n",
    "  Note on Anomaly Detection: It's important to note that our anomaly detection approach assumes the exclusive knowledge of the normal class. The detection relies heavily on the loss statistics of the normal class, with anomalous classes not being directly used in training. However, the inclusion of random noise in training serves as a supplementary feature to enhance learning.\n",
    "\n",
    "4. Metrics for Explainers:\n",
    "As a key part of this assignment, you will calculate and analyze various metrics for selected explainers. These explainers can be rule-based or importance-based, such as SHAP (SHapley Additive exPlanations). The metrics to focus on include:\n",
    "- Stability: Evaluating how consistent the explainer's output is across different runs or slight variations in input.\n",
    "- Fidelity (excluding SHAP): Assessing how accurately the explanations reflect the behaviors of the underlying models.\n",
    "- Consistency: This involves applying the explainer to the same instance multiple times (e.g., five times using LIME) and observing the variation in explanations.\n",
    "\n",
    "This practical exercise aims to enhance understanding of how different explainers operate and to evaluate their effectiveness in making AI models more interpretable and transparent. Let's begin our journey towards making AI more explainable and trustworthy!"
   ],
   "metadata": {
    "id": "ViKbQwc00b_v"
   },
   "id": "ViKbQwc00b_v"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Iris dataset and model"
   ],
   "metadata": {
    "id": "YpLDORn4RiKg"
   },
   "id": "YpLDORn4RiKg"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "! mkdir serialised\n",
    "! cd serialised && rm Iris_contrastive_model_versicolor_v9_noise_improved.pht\n",
    "! cd serialised && rm Iris_contrastive_model_versicolor_v9_noise_improved__colab_version_3.pht\n",
    "! cd serialised && wget https://github.com/mozo64/xai-evaluation-classes/raw/main/serialised/Iris_contrastive_model_versicolor_v9_noise_improved.pht\n",
    "! cd serialised && wget https://github.com/mozo64/xai-evaluation-classes/raw/main/serialised/Iris_contrastive_model_versicolor_v9_noise_improved__colab_version_3.pht\n",
    "\n",
    "! cd serialised && ls"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f4259e000fcf26be",
    "outputId": "33122a11-0b02-455f-fc2c-93e3de5f49a1"
   },
   "id": "f4259e000fcf26be"
  },
  {
   "cell_type": "code",
   "source": [
    "! pip install dill shap lime anchor-exp gower imbalanced-learn numdifftools\n",
    "! apt-get install graphviz\n",
    "\n",
    "! git clone -b main --single-branch https://github.com/sbobek/lux.git\n",
    "%cd lux\n",
    "! git submodule update --init --recursive\n",
    "%cd .."
   ],
   "metadata": {
    "id": "RM8G6zW-lYzx",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ea26d0e3-47ec-41c4-904f-9395c8a51ecd",
    "ExecuteTime": {
     "end_time": "2023-12-19T09:39:50.024509419Z",
     "start_time": "2023-12-19T09:39:40.387714656Z"
    }
   },
   "id": "RM8G6zW-lYzx",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dill in ./.conda/lib/python3.11/site-packages (0.3.7)\r\n",
      "Requirement already satisfied: shap in ./.conda/lib/python3.11/site-packages (0.42.1)\r\n",
      "Requirement already satisfied: lime in ./.conda/lib/python3.11/site-packages (0.2.0.1)\r\n",
      "Requirement already satisfied: anchor-exp in ./.conda/lib/python3.11/site-packages (0.0.2.0)\r\n",
      "Requirement already satisfied: gower in ./.conda/lib/python3.11/site-packages (0.1.2)\r\n",
      "Requirement already satisfied: imbalanced-learn in ./.conda/lib/python3.11/site-packages (0.11.0)\r\n",
      "Requirement already satisfied: numdifftools in ./.conda/lib/python3.11/site-packages (0.9.41)\r\n",
      "Requirement already satisfied: numpy in ./.conda/lib/python3.11/site-packages (from shap) (1.24.3)\r\n",
      "Requirement already satisfied: scipy in ./.conda/lib/python3.11/site-packages (from shap) (1.9.3)\r\n",
      "Requirement already satisfied: scikit-learn in ./.conda/lib/python3.11/site-packages (from shap) (1.2.2)\r\n",
      "Requirement already satisfied: pandas in ./.conda/lib/python3.11/site-packages (from shap) (1.5.3)\r\n",
      "Requirement already satisfied: tqdm>=4.27.0 in ./.conda/lib/python3.11/site-packages (from shap) (4.66.1)\r\n",
      "Requirement already satisfied: packaging>20.9 in ./.conda/lib/python3.11/site-packages (from shap) (23.1)\r\n",
      "Requirement already satisfied: slicer==0.0.7 in ./.conda/lib/python3.11/site-packages (from shap) (0.0.7)\r\n",
      "Requirement already satisfied: numba in ./.conda/lib/python3.11/site-packages (from shap) (0.57.1)\r\n",
      "Requirement already satisfied: cloudpickle in ./.conda/lib/python3.11/site-packages (from shap) (3.0.0)\r\n",
      "Requirement already satisfied: matplotlib in ./.conda/lib/python3.11/site-packages (from lime) (3.8.2)\r\n",
      "Requirement already satisfied: scikit-image>=0.12 in ./.conda/lib/python3.11/site-packages (from lime) (0.21.0)\r\n",
      "Requirement already satisfied: spacy in ./.conda/lib/python3.11/site-packages (from anchor-exp) (3.7.2)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in ./.conda/lib/python3.11/site-packages (from imbalanced-learn) (1.2.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.conda/lib/python3.11/site-packages (from imbalanced-learn) (3.2.0)\r\n",
      "Requirement already satisfied: networkx>=2.8 in ./.conda/lib/python3.11/site-packages (from scikit-image>=0.12->lime) (3.2.1)\r\n",
      "Requirement already satisfied: pillow>=9.0.1 in ./.conda/lib/python3.11/site-packages (from scikit-image>=0.12->lime) (9.5.0)\r\n",
      "Requirement already satisfied: imageio>=2.27 in ./.conda/lib/python3.11/site-packages (from scikit-image>=0.12->lime) (2.32.0)\r\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in ./.conda/lib/python3.11/site-packages (from scikit-image>=0.12->lime) (2023.9.26)\r\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in ./.conda/lib/python3.11/site-packages (from scikit-image>=0.12->lime) (1.5.0)\r\n",
      "Requirement already satisfied: lazy_loader>=0.2 in ./.conda/lib/python3.11/site-packages (from scikit-image>=0.12->lime) (0.3)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.conda/lib/python3.11/site-packages (from matplotlib->lime) (1.2.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in ./.conda/lib/python3.11/site-packages (from matplotlib->lime) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.conda/lib/python3.11/site-packages (from matplotlib->lime) (4.44.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.conda/lib/python3.11/site-packages (from matplotlib->lime) (1.4.5)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.conda/lib/python3.11/site-packages (from matplotlib->lime) (3.1.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.conda/lib/python3.11/site-packages (from matplotlib->lime) (2.8.2)\r\n",
      "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in ./.conda/lib/python3.11/site-packages (from numba->shap) (0.40.1)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.conda/lib/python3.11/site-packages (from pandas->shap) (2023.3)\r\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in ./.conda/lib/python3.11/site-packages (from spacy->anchor-exp) (3.0.12)\r\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./.conda/lib/python3.11/site-packages (from spacy->anchor-exp) (1.0.5)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./.conda/lib/python3.11/site-packages (from spacy->anchor-exp) (1.0.10)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./.conda/lib/python3.11/site-packages (from spacy->anchor-exp) (2.0.8)\r\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./.conda/lib/python3.11/site-packages (from spacy->anchor-exp) (3.0.9)\r\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in ./.conda/lib/python3.11/site-packages (from spacy->anchor-exp) (8.2.1)\r\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in ./.conda/lib/python3.11/site-packages (from spacy->anchor-exp) (1.1.2)\r\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in ./.conda/lib/python3.11/site-packages (from spacy->anchor-exp) (2.4.8)\r\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./.conda/lib/python3.11/site-packages (from spacy->anchor-exp) (2.0.10)\r\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in ./.conda/lib/python3.11/site-packages (from spacy->anchor-exp) (0.3.4)\r\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in ./.conda/lib/python3.11/site-packages (from spacy->anchor-exp) (0.9.0)\r\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in ./.conda/lib/python3.11/site-packages (from spacy->anchor-exp) (6.4.0)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./.conda/lib/python3.11/site-packages (from spacy->anchor-exp) (2.25.1)\r\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in ./.conda/lib/python3.11/site-packages (from spacy->anchor-exp) (2.0.3)\r\n",
      "Requirement already satisfied: jinja2 in ./.conda/lib/python3.11/site-packages (from spacy->anchor-exp) (3.1.2)\r\n",
      "Requirement already satisfied: setuptools in ./.conda/lib/python3.11/site-packages (from spacy->anchor-exp) (68.0.0)\r\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./.conda/lib/python3.11/site-packages (from spacy->anchor-exp) (3.3.0)\r\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./.conda/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->anchor-exp) (0.6.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.3.0 in ./.conda/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->anchor-exp) (2.3.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in ./.conda/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->anchor-exp) (4.8.0)\r\n",
      "Requirement already satisfied: six>=1.5 in ./.conda/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib->lime) (1.16.0)\r\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in ./.conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy->anchor-exp) (4.0.0)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in ./.conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy->anchor-exp) (2.10)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./.conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy->anchor-exp) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy->anchor-exp) (2023.7.22)\r\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in ./.conda/lib/python3.11/site-packages (from thinc<8.3.0,>=8.1.8->spacy->anchor-exp) (0.7.11)\r\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./.conda/lib/python3.11/site-packages (from thinc<8.3.0,>=8.1.8->spacy->anchor-exp) (0.1.4)\r\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in ./.conda/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy->anchor-exp) (8.1.7)\r\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in ./.conda/lib/python3.11/site-packages (from weasel<0.4.0,>=0.1.0->spacy->anchor-exp) (0.16.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.conda/lib/python3.11/site-packages (from jinja2->spacy->anchor-exp) (2.1.3)\r\n",
      "E: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\r\n",
      "E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\r\n",
      "Cloning into 'lux'...\r\n",
      "remote: Enumerating objects: 464, done.\u001B[K\r\n",
      "remote: Counting objects: 100% (166/166), done.\u001B[K\r\n",
      "remote: Compressing objects: 100% (96/96), done.\u001B[K\r\n",
      "remote: Total 464 (delta 111), reused 102 (delta 70), pack-reused 298\u001B[K\r\n",
      "Receiving objects: 100% (464/464), 96.41 MiB | 25.92 MiB/s, done.\r\n",
      "Resolving deltas: 100% (223/223), done.\r\n",
      "/home/jovyan/lux\n",
      "Submodule 'pyuid3' (https://github.com/sbobek/pyuid3) registered for path 'pyuid3'\r\n",
      "Cloning into '/home/jovyan/lux/pyuid3'...\r\n",
      "Submodule path 'pyuid3': checked out '4970732d56f50ae39c2e62b2f607faabea520415'\r\n",
      "/home/jovyan\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: -c: line 1: syntax error near unexpected token `then'\r\n",
      "/bin/bash: -c: line 1: ` if[-f auxilliary_eval_classes.py]; then rm auxilliary_eval_classes.py; fi'\r\n",
      "--2023-12-19 09:43:08--  https://raw.githubusercontent.com/mozo64/xai-evaluation-classes/main/auxilliary_eval_classes.py\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 16129 (16K) [text/plain]\r\n",
      "Saving to: ‘auxilliary_eval_classes.py’\r\n",
      "\r\n",
      "auxilliary_eval_cla 100%[===================>]  15.75K  --.-KB/s    in 0.002s  \r\n",
      "\r\n",
      "2023-12-19 09:43:08 (6.22 MB/s) - ‘auxilliary_eval_classes.py’ saved [16129/16129]\r\n",
      "\r\n",
      "X_anomaly_1_test_df = anomaly_1_gauss_df[\r\n",
      "    ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']].to_numpy()\r\n",
      "X_anomaly_2_test_df = anomaly_2_gauss_df[\r\n",
      "    ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']].to_numpy()\r\n",
      "print(f\"X_normal # = {len(X_normal_test_df)}\")\r\n",
      "print(f\"X_anomaly_1 # = {len(X_anomaly_1_test_df)}\")\r\n",
      "print(f\"X_anomaly_2 # = {len(X_anomaly_2_test_df)}\")\r\n",
      "\r\n",
      "\r\n",
      "## 16/12/2023 20:53 ###\r\n",
      "ANOMALY_CLASS_1 is setosa\n",
      "ANOMALY_CLASS_2 is virginica\n",
      "X_normal # = 500\n",
      "X_anomaly_1 # = 500\n",
      "X_anomaly_2 # = 500\n"
     ]
    }
   ],
   "source": [
    "! if[-f auxilliary_eval_classes.py]; then rm auxilliary_eval_classes.py; fi\n",
    "! wget -O auxilliary_eval_classes.py https://raw.githubusercontent.com/mozo64/xai-evaluation-classes/main/auxilliary_eval_classes.py\n",
    "\n",
    "! tail auxilliary_eval_classes.py\n",
    "\n",
    "# Importowanie modułu\n",
    "# from importlib import reload\n",
    "\n",
    "# import auxilliary_eval_classes\n",
    "\n",
    "# Ponowne załadowanie modułu\n",
    "# auxiliary_mushrooms = reload(auxilliary_eval_classes)\n",
    "\n",
    "from auxilliary_eval_classes import iris_df, iris, print_loss, \\\n",
    "      normal_train_df, uniform_df_3, \\\n",
    "      X_normal_test_df, X_anomaly_1_test_df, X_anomaly_2_test_df, scaler, \\\n",
    "      NORMAL_CLASS, ANOMALY_CLASS_1, ANOMALY_CLASS_2, \\\n",
    "      get_losses_impl, assess_gaussianity_and_percentile, \\\n",
    "      IrisAutoencoder3, AnomalyClassifier\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "np.int = int  # Fix deprecated\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "import graphviz\n",
    "from IPython.display import Image"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "99b91553d8edb624",
    "outputId": "f48d14fd-4372-4b6c-f685-cf79e341176e",
    "ExecuteTime": {
     "end_time": "2023-12-19T09:43:09.788744825Z",
     "start_time": "2023-12-19T09:43:07.882065544Z"
    }
   },
   "id": "16d9a17f21634399"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%cd lux\n",
    "! sed -i '25ifrom pyuid3.uncertain_entropy_evaluator import UncertainEntropyEvaluator' lux/lux.py\n",
    "\n",
    "from lux.lux import LUX\n",
    "%cd .."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "99b91553d8edb624"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Iris dataset"
   ],
   "metadata": {
    "id": "73Ks0dCHwTYh"
   },
   "id": "73Ks0dCHwTYh"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Original dataset"
   ],
   "metadata": {
    "id": "1uoiMFcU-efF"
   },
   "id": "1uoiMFcU-efF"
  },
  {
   "cell_type": "code",
   "source": [
    "# Plot the pairwise relationships in the dataset\n",
    "sns_plot = sns.pairplot(iris_df, vars=iris['feature_names'], hue=\"species\")\n",
    "\n",
    "# Move the legend on top of the plot\n",
    "handles = sns_plot._legend_data.values()\n",
    "labels = sns_plot._legend_data.keys()\n",
    "sns_plot._legend.remove()\n",
    "sns_plot.fig.legend(handles=handles, labels=labels, loc='upper right', ncol=1)\n",
    "sns_plot._legend.set_bbox_to_anchor((0.9, 0.6))  # adjust the values as needed\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "vr5lPU1ImXPh",
    "outputId": "1ce38b36-5f3e-4429-d4c7-41547cf79a0e",
    "ExecuteTime": {
     "start_time": "2023-12-19T09:38:17.485222292Z"
    }
   },
   "id": "vr5lPU1ImXPh",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data on which Autoencoder was trained"
   ],
   "metadata": {
    "id": "3kyaEAqcBJmn"
   },
   "id": "3kyaEAqcBJmn"
  },
  {
   "cell_type": "code",
   "source": [
    "normal_train_df.head()  # Versicolor - augmented samples\n",
    "uniform_df_3.head()  # uniform noise - used in training"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "rOY-ihrHBCee",
    "outputId": "ff686592-9f61-416c-efa9-6b938557532d",
    "ExecuteTime": {
     "start_time": "2023-12-19T09:38:17.488231221Z"
    }
   },
   "id": "rOY-ihrHBCee",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "scaler  # scaler was fitted on normal_train_df"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "hInY8jOHCHTk",
    "outputId": "4af4898b-78b8-468a-8a41-0a3637bc48c6",
    "ExecuteTime": {
     "start_time": "2023-12-19T09:38:17.514301550Z"
    }
   },
   "id": "hInY8jOHCHTk",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "_feature_columns = ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
    "\n",
    "X_normal_train_scaled = scaler.transform(normal_train_df[_feature_columns].to_numpy())  # Use scaler\n",
    "X_normal_train_scaled_pt = torch.tensor(X_normal_train_scaled, dtype=torch.float32)"
   ],
   "metadata": {
    "id": "SPa5EhSVKiUq",
    "ExecuteTime": {
     "start_time": "2023-12-19T09:38:17.515456852Z"
    }
   },
   "id": "SPa5EhSVKiUq",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test set"
   ],
   "metadata": {
    "id": "766z2TiV-pgO"
   },
   "id": "766z2TiV-pgO"
  },
  {
   "cell_type": "code",
   "source": [
    "X_normal_test_df  # Normal class (Versicolor)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DQmx31a--0Ae",
    "outputId": "e17eb6a9-87bc-4714-887a-8aca92290695",
    "ExecuteTime": {
     "start_time": "2023-12-19T09:38:17.516104994Z"
    }
   },
   "id": "DQmx31a--0Ae",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "X_normal_scaled = scaler.transform(X_normal_test_df)  # Use scaler\n",
    "X_normal_scaled_pt = torch.FloatTensor(X_normal_scaled)"
   ],
   "metadata": {
    "id": "vGSOJuBC_vhh",
    "ExecuteTime": {
     "start_time": "2023-12-19T09:38:17.516721487Z"
    }
   },
   "id": "vGSOJuBC_vhh",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Anonaly classes\n",
    "X_anomaly_1_scaled = scaler.transform(X_anomaly_1_test_df)  # Setosa\n",
    "X_anomaly_2_scaled = scaler.transform(X_anomaly_2_test_df)  # Virginica\n",
    "X_anomaly_1_scaled_pt = torch.FloatTensor(X_anomaly_1_scaled)\n",
    "X_anomaly_2_scaled_pt = torch.FloatTensor(X_anomaly_2_scaled)"
   ],
   "metadata": {
    "id": "VXFmCb2sHQt9",
    "ExecuteTime": {
     "start_time": "2023-12-19T09:38:17.518010298Z"
    }
   },
   "id": "VXFmCb2sHQt9",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Autoencoder model"
   ],
   "metadata": {
    "id": "qmwkn-Qi12Qt"
   },
   "id": "qmwkn-Qi12Qt"
  },
  {
   "cell_type": "code",
   "source": [
    "# We skip training and load the model\n",
    "model_iris = IrisAutoencoder3()\n",
    "# If file cannot be loaded, it means it is corrupred an need to be downloaded once agian - see beginning of the notebook\n",
    "model_iris.load_state_dict(torch.load('serialised/Iris_contrastive_model_versicolor_v9_noise_improved.pht'))\n",
    "\n",
    "model_iris.eval()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FtSPZhrSnoCl",
    "outputId": "4aadd28b-e051-4947-fc38-78326a9298f0",
    "ExecuteTime": {
     "start_time": "2023-12-19T09:38:17.518691485Z"
    }
   },
   "id": "FtSPZhrSnoCl",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "### 3. Test autoencoder, visualise losses\n",
    "print_loss(X_normal_scaled_pt, model_iris, f\"Norm= {NORMAL_CLASS}\")\n",
    "print_loss(X_anomaly_1_scaled_pt, model_iris, f\"Anomal1= {ANOMALY_CLASS_1}\")\n",
    "print_loss(X_anomaly_2_scaled_pt, model_iris, f\"Anomal2= {ANOMALY_CLASS_2}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TEvmRXK4F9Uf",
    "outputId": "05f29d29-bda3-450c-9bc2-a1f9c167a6f0",
    "ExecuteTime": {
     "start_time": "2023-12-19T09:38:17.519364528Z"
    }
   },
   "id": "TEvmRXK4F9Uf",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Visualise losses from Autoencoder\n",
    "losses_normal_test, _ = get_losses_impl(X_normal_scaled_pt, model_iris, f\"Norm= {NORMAL_CLASS}\")\n",
    "losses_anomaly_1_test, _ = get_losses_impl(X_anomaly_1_scaled_pt, model_iris, f\"Anomal1= {ANOMALY_CLASS_1}\")\n",
    "losses_anomaly_2_test, _ = get_losses_impl(X_anomaly_2_scaled_pt, model_iris, f\"Anomal2= {ANOMALY_CLASS_2}\")\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.hist(losses_normal_test, bins=30, label=f\"Norm= {NORMAL_CLASS}\", alpha=0.7)\n",
    "plt.hist(losses_anomaly_1_test, bins=30, label=f\"Anomal1= {ANOMALY_CLASS_1}\", alpha=0.3)\n",
    "plt.hist(losses_anomaly_2_test, bins=30, label=f\"Anomal2= {ANOMALY_CLASS_2}\", alpha=0.3)\n",
    "\n",
    "plt.title('Distribution of Losses')\n",
    "plt.xlabel('Loss')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 770
    },
    "id": "fc4CQx8BIYg5",
    "outputId": "741208fe-8b7a-440b-9c4b-729219e53946",
    "ExecuteTime": {
     "start_time": "2023-12-19T09:38:17.520724809Z"
    }
   },
   "id": "fc4CQx8BIYg5",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fit AnomalyClassifier - it is wrapper fo Autoencoder"
   ],
   "metadata": {
    "id": "NM2c4_H6DeSu"
   },
   "id": "NM2c4_H6DeSu"
  },
  {
   "cell_type": "code",
   "source": [
    "# Classifier is initialised soley on train - Normal class (Versicolor)\n",
    "losses_normal_train, _ = get_losses_impl(X_normal_train_scaled_pt, model_iris, f\"Norm= {NORMAL_CLASS}\")\n",
    "\n",
    "treshold_90, assessment = assess_gaussianity_and_percentile(losses_normal_train, 0.90)\n",
    "print(f\"Value at 90th percentile: {treshold_90:.2f}\")\n",
    "print(assessment)\n",
    "\n",
    "\n",
    "model_classifier = AnomalyClassifier(autoencoder_model=model_iris,\n",
    "                                     get_losses=get_losses_impl,\n",
    "                                     loss_threshold=treshold_90,\n",
    "                                     scaler=scaler)\n",
    "\n",
    "# Predict proba is based on probability of loss being draw from distribution for train (Normal class - Versivolor)\n",
    "model_classifier.initialise_predict_proba(X_normal_train_scaled_pt)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 921
    },
    "id": "04ayEoCGEsMn",
    "outputId": "6a7f07f5-6a26-423e-fb58-94c7f3c5e624",
    "ExecuteTime": {
     "start_time": "2023-12-19T09:38:17.521415403Z"
    }
   },
   "id": "04ayEoCGEsMn",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# How to get predictions from AnomalyClassifier on original data"
   ],
   "metadata": {
    "id": "jzLD61ma4bxd"
   },
   "id": "jzLD61ma4bxd"
  },
  {
   "cell_type": "code",
   "source": [
    "# Prepare data from iris_df\n",
    "_feature_names = [n.replace(' ', '_').replace('(', '').replace(')', '') for n in iris_df.columns]\n",
    "print(\"Features names:\", \", \".join(_feature_names))\n",
    "iris_df_cp = iris_df.copy()\n",
    "iris_df_cp.columns = _feature_names\n",
    "iris_df_cp.tail()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "bUAx6CtAnFdR",
    "outputId": "a945ca5f-829a-405e-e48f-120492c29a2a",
    "ExecuteTime": {
     "start_time": "2023-12-19T09:38:17.545975008Z"
    }
   },
   "id": "bUAx6CtAnFdR",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# # !!! Model predicts only 0 - normal and 1 - anomaly\n",
    "predictions = model_classifier.predict(iris_df_cp[_feature_names[:-2]])\n",
    "\n",
    "# Predict proba\n",
    "probas = model_classifier.predict_proba(iris_df_cp[_feature_names[:-2]])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HedJG8-emQDe",
    "outputId": "37596059-e3ba-4444-f1e6-a2dd9f6e7ff1",
    "ExecuteTime": {
     "start_time": "2023-12-19T09:38:17.546462378Z"
    }
   },
   "id": "HedJG8-emQDe",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "predictions[-15:]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bH5mK9b63FA6",
    "outputId": "21f3f4a3-6798-43a3-c5e6-a4f21ac46fb4",
    "ExecuteTime": {
     "start_time": "2023-12-19T09:38:17.546874877Z"
    }
   },
   "id": "bH5mK9b63FA6",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "probas[-15:]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TGDxoWFW3Hxg",
    "outputId": "632b1d08-0c3c-4e3a-a188-bc0b01b25e1f",
    "ExecuteTime": {
     "start_time": "2023-12-19T09:38:17.547163899Z"
    }
   },
   "id": "TGDxoWFW3Hxg",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Your task starts here\n",
    "\n"
   ],
   "metadata": {
    "id": "bziOQRAsxQmE"
   },
   "id": "bziOQRAsxQmE"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Explainable AI Assignment: Anomaly Detection with SHAP/LIME and LUX\n",
    "\n",
    "## Overview\n",
    "In this assignment, you will use the AnomalyClassifier, which predicts two classes: 0 (normal) and 1 (anomaly). Your task is to explain the predictions of this classifier using either SHAP, LIME, or LUX explainer. You will then evaluate the explanations based on three key metrics: Stability, Fidelity, and Consistency.\n",
    "\n",
    "## Objectives\n",
    "1. Apply a) SHAP or LIME and b) LUX to provide explanations for the AnomalyClassifier.\n",
    "2. Calculate and analyze the following metrics for the chosen explainer:\n",
    "   - Stability of Explanations\n",
    "   - Fidelity of Explanations\n",
    "   - Consistency\n",
    "\n",
    "## Metrics Formulas\n",
    "Here are the formulas for the metrics you need to compute:\n",
    "\n",
    "### **1. Stability of Explanations**\n",
    "Stability measures how minor changes in the instances being explained affect changes in the explanations. A stable explainer will show minimal differences in explanations for instances with small differences, especially if the model's prediction for these instances remains the same. Lower variance in explanations across different runs indicates higher stability.\n",
    "\n",
    "Stability in this context is measured using local Lipschitz continuity in a fixed neighborhood of any datapoint. This approach evaluates how similar the explanations are for similar inputs. A lower value indicates higher stability, implying that small changes in input do not significantly alter the explanation.\n",
    "\n",
    "$$\n",
    "\\text{Stability}_{\\text{Lipschitz}} = \\max_{i \\neq j} \\frac{\\| \\text{explanation}(x_i) - \\text{explanation}(x_j) \\|}{\\| x_i - x_j \\|}\n",
    "$$\n",
    "\n",
    "where \\(||...||) denotes a norm (e.g., Euclidean), \\( xi \\) and \\( xj \\) are datapoints within the fixed neighborhood, and \\(explanation(x)) is the explainer's output for datapoint \\( x \\).\n",
    "\n",
    "\n",
    "### **2. Fidelity of Explanations**\n",
    "Fidelity evaluates how closely the explainer's predictions align with the actual model predictions. It's calculated for 'n' instances.\n",
    "\n",
    "The formula for Fidelity is as follows:\n",
    "\n",
    "$$\n",
    "\\text{Fidelity} = \\frac{1}{n} \\sum_{i=1}^{n} \\left(1 - |\\text{prediction}_{i} - \\text{explanation_prediction}_{i}|\\right)\n",
    "$$\n",
    "\n",
    "  - `prediction_i`: Represents the actual prediction made by the model for the i-th instance.\n",
    "  - `explanation_prediction_i`: Is the prediction given by the explainer for the same i-th instance.\n",
    "  - `|prediction_i - explanation_prediction_i|`: The absolute difference between the model's prediction and the explainer's prediction measures the level of disagreement. Smaller differences indicate higher fidelity.\n",
    "  - `1 - |prediction_i - explanation_prediction_i|`: This expression gives us a measure of agreement. A fidelity score close to 1 indicates a high level of agreement, suggesting that the explainer's predictions closely match those of the model.\n",
    "\n",
    "  #### Applicability of the Method:\n",
    "\n",
    "  This method of calculating fidelity is generally applicable to a range of explainers. However, it is important to note that the effectiveness of this metric can vary depending on the nature of the explainer and the model. With SHAP, which provides feature importance as explanations rather than direct predictions, the calculation of fidelity need to be adapted.\n",
    "  ##### **Fidelity Calculation for SHAP Explainers**\n",
    "\n",
    "    Fidelity in the context of SHAP explainers is calculated differently due to SHAP providing feature importances rather than direct predictions. The following method can be used to adapt fidelity calculation for SHAP:\n",
    "\n",
    "    1. **Model Predictions:** Use your model to generate predictions for a dataset.\n",
    "\n",
    "    2. **SHAP Explanations:** Generate SHAP feature importance explanations for the same instances.\n",
    "\n",
    "    3. **Modify Input Data:** For each instance, modify the input data by altering the most important features according to SHAP (e.g., setting them to zero or replacing them with their mean value).\n",
    "\n",
    "    4. **Re-Predict and Compare:** Use the model to predict again for these modified instances and compare these predictions with the original ones.\n",
    "\n",
    "    5. **Calculate Fidelity:** Fidelity is calculated as a measure of how much predictions change after modifying the most important features identified by SHAP.\n",
    "\n",
    "    Example Formula:\n",
    "\n",
    "  $$\n",
    "  \\text{Fidelity}_{\\text{SHAP}} = \\frac{1}{n} \\sum_{i=1}^{n} \\left(1 - \\text{change_in_prediction}(x_i, x'_i)\\right)\n",
    "  $$\n",
    "\n",
    "    Where `xi` is the original instance, and `x'i` is the modified instance with changes based on SHAP feature importances. The function `change_in_prediction` measures the extent of change in the prediction between these two instances.\n",
    "\n",
    "    This method of calculating fidelity for SHAP takes into account the impact of the most important features on the model's predictions, assessing how faithfully these importances reflect the model's behavior.\n",
    "\n",
    "\n",
    "### **3. Consistency**\n",
    "Consistency assesses the variance in explanations for the same instance across multiple runs. Lower variance signifies higher consistency.\n",
    "\n",
    "The formula for Consistency is as follows:\n",
    "\n",
    "$$\n",
    "\\text{Consistency} = 1 - \\frac{\\sum_{i=1}^{n} \\text{variance}(\\text{explanations}_{i})}{n}\n",
    "$$\n",
    "\n",
    "  - `variance(explanations_i)`: This represents the variance in the explanations generated for the i-th instance across different runs. A high variance indicates that the explainer's output varies significantly for the same instance, suggesting lower consistency.\n",
    "  - The average of these variances over 'n' instances gives an overall measure of the explainer's consistency.\n",
    "  - A higher consistency score (closer to 1) means that the explainer provides stable and reliable explanations across multiple evaluations.\n",
    "\n",
    "## Instructions\n",
    "1. Choose either SHAP or LIME and LUX as your explainer tool.\n",
    "2. Apply the chosen explainer to the AnomalyClassifier to generate explanations for a set of instances.\n",
    "3. Compute the Stability, Fidelity, and Consistency of the explanations using the above formulas.\n",
    "4. Analyze the results to understand the effectiveness of your chosen explainer in making the AnomalyClassifier's decisions interpretable.\n",
    "\n",
    "### General Note on Metric Convergence\n",
    "\n",
    "For each of the metrics - Stability, Fidelity, and Consistency - it's crucial to aim for convergence over successive iterations. This means that with each iteration, the change in the metric's value should become progressively smaller. Achieving convergence indicates that the explainer's performance is consistent and reliable. If the metrics do not converge, it may suggest that the explainer's output is highly sensitive to small changes in the data or the modeling process, which can be a concern for the robustness of the explainer.\n",
    "\n",
    "Remember, the goal of this assignment is to explore how explainable AI tools can help in understanding and interpreting machine learning models, particularly in anomaly detection scenarios.\n"
   ],
   "metadata": {
    "id": "zK2d_Vm8ZeHc"
   },
   "id": "zK2d_Vm8ZeHc"
  },
  {
   "cell_type": "code",
   "source": [
    "iris_df_cp[\"species\"].value_counts()"
   ],
   "metadata": {
    "id": "SY_Q8gh3crzp",
    "ExecuteTime": {
     "start_time": "2023-12-19T09:38:17.547462777Z"
    }
   },
   "id": "SY_Q8gh3crzp",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train_lux = iris_df_cp[['sepal_length_cm', 'sepal_width_cm', 'petal_length_cm', 'petal_width_cm']].copy()\n",
    "y_train_lux = iris_df_cp[['species']].copy()\n",
    "\n",
    "# y_train_lux jest serią pandas\n",
    "y_train_lux_encoded = y_train_lux.replace({'versicolor': 0, 'setosa': 1, 'virginica': 1})\n",
    "y_train_lux_encoded.value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-19T09:38:17.547745533Z"
    }
   },
   "id": "6ad9452b4d00313b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_classifier.predict_proba(X_train_lux[40:60])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-19T09:38:17.548063163Z"
    }
   },
   "id": "642c3e999d9e97a2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "idx = 0\n",
    "i2e = X_train_lux.iloc[idx].values.reshape(1, -1)\n",
    "lux_obj = LUX(\n",
    "          predict_proba=model_classifier.predict_proba,\n",
    "          #classifier=model_classifier, #Uncomment to use SHAP=based sampling. This will take long time as the SHAP values will have to be recalculated for every split\n",
    "          neighborhood_size=0.1, max_depth=3, node_size_limit=4, grow_confidence_threshold=0, min_samples=50)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-19T09:38:17.548389450Z"
    }
   },
   "id": "672fc8acafa7f9f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lux_obj.fit(X_train_lux, y_train_lux_encoded, instance_to_explain=i2e, inverse_sampling=True,\n",
    "        oversampling=False,\n",
    "        #Change to True i f you want to generate samples (note -- in current version is will take very long time to compute)\n",
    "        prune=True, oblique=False)\n",
    "lux_obj"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-19T09:38:17.548665349Z"
    }
   },
   "id": "f47dbb4e31a30dd7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lux_obj.justify(i2e, to_dict=True)\n",
    "# [[{'rule': {'sepal_length_cm': ['<5.35']},\n",
    "#    'prediction': '1',\n",
    "#    'confidence': 1.0}]]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-19T09:38:17.549010881Z"
    }
   },
   "id": "9fc8aa23c22ac0ae"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lux_cf = lux_obj.counterfactual(i2e, background=X_train_lux, counterfactual_representative='nearest', topn=1)[0]\n",
    "lux_cf['rule']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-19T09:38:17.549292738Z"
    }
   },
   "id": "31dd9dae1189b5f8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "i2edf = pd.DataFrame(i2e, columns=X_train_lux.columns)\n",
    "lux_cf_nearest = lux_cf['counterfactual'].to_frame().T\n",
    "lux_cf_nearest['class'] = model_classifier.predict(lux_cf_nearest)\n",
    "i2edf['class'] = model_classifier.predict(i2edf)\n",
    "\n",
    "lux_obj.uid3.tree.save_dot('tree-pure.dot', fmt='.2f')\n",
    "graphviz.Source.from_file('tree-pure.dot')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-19T09:38:17.549592705Z"
    }
   },
   "id": "a76b2e7a2664e160"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train_lux_bg = X_train_lux.copy()\n",
    "X_train_lux_bg['class'] = y_train_lux_encoded\n",
    "lux_obj.uid3.tree.save_dot('tree-cf-visual.dot', fmt='.2f', visual=True, background_data=X_train_lux_bg, instance2explain=i2edf,\n",
    "                       counterfactual=lux_cf_nearest)\n",
    "gvz = graphviz.Source.from_file('tree-cf-visual.dot')\n",
    "!dot -Tpng./tree-cf-visual.dot > tree.png\n",
    "Image('tree.png')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-19T09:38:17.574988144Z"
    }
   },
   "id": "78cbce053c941df8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
